diff --git a/estimate_normals.py b/estimate_normals.py
new file mode 100644
index 0000000..1fe9032
--- /dev/null
+++ b/estimate_normals.py
@@ -0,0 +1,139 @@
+import torch
+import torch.nn.functional as F
+from torchvision import transforms
+
+import PIL
+from PIL import Image
+import numpy as np
+import matplotlib.pyplot as plt
+
+import argparse
+import os.path
+from pathlib import Path
+import glob
+import sys
+from tqdm import tqdm
+
+from omnidata_tools.torch.modules.midas.dpt_depth import DPTDepthModel
+from omnidata_tools.torch.data.transforms import get_transform
+
+parser = argparse.ArgumentParser(description='Visualize output for depth or surface normals')
+parser.add_argument('--task', dest='task', help="normal or depth")
+parser.set_defaults(task='NONE')
+parser.add_argument('--img_path', dest='img_path', help="path to rgb image")
+parser.set_defaults(im_name='NONE')
+parser.add_argument('--output_path', dest='output_path', help="path to where output image should be stored")
+parser.set_defaults(store_name='NONE')
+
+args = parser.parse_args()
+
+root_dir = './pretrained_models/'
+
+trans_topil = transforms.ToPILImage()
+
+if args.task == 'NONE':
+    args.task = 'normal'
+if args.output_path is None:
+    args.output_path = f"{args.img_path}/../{args.task}"
+os.makedirs(args.output_path, exist_ok=True)
+map_location = (lambda storage, loc: storage.cuda()) if torch.cuda.is_available() else torch.device('cpu')
+device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
+
+if args.task == 'normal':
+    pretrained_weights_path = root_dir + 'omnidata_dpt_normal_v2.ckpt'
+    model = DPTDepthModel(backbone='vitb_rn50_384', num_channels=3) # DPT Hybrid
+    checkpoint = torch.load(pretrained_weights_path, map_location=map_location)
+    if 'state_dict' in checkpoint:
+        state_dict = {}
+        for k, v in checkpoint['state_dict'].items():
+            state_dict[k[6:]] = v
+    else:
+        state_dict = checkpoint
+    model.load_state_dict(state_dict)
+    model.to(device)
+
+elif args.task == 'depth':
+    pretrained_weights_path = root_dir + 'omnidata_dpt_depth_v2.ckpt'  # 'omnidata_dpt_depth_v1.ckpt'
+    model = DPTDepthModel(backbone='vitb_rn50_384') # DPT Hybrid
+    checkpoint = torch.load(pretrained_weights_path, map_location=map_location)
+    if 'state_dict' in checkpoint:
+        state_dict = {}
+        for k, v in checkpoint['state_dict'].items():
+            state_dict[k[6:]] = v
+    else:
+        state_dict = checkpoint
+    model.load_state_dict(state_dict)
+    model.to(device)
+
+
+@torch.no_grad()
+def save_outputs(img_path, output_file_name):
+    img = Image.open(img_path)
+
+    WIDTH, HEIGHT = img.size
+    INPUT_WIDTH = WIDTH // 32 * 32
+    INPUT_HEIGHT = HEIGHT // 32 * 32
+
+    if args.task == 'depth':
+        trans_totensor = transforms.Compose([transforms.Resize([INPUT_HEIGHT, INPUT_WIDTH], interpolation=PIL.Image.BILINEAR),
+                                            transforms.ToTensor(),
+                                            transforms.Normalize(mean=0.5, std=0.5)])
+    elif args.task == 'normal':
+        trans_totensor = transforms.Compose([transforms.Resize([INPUT_HEIGHT, INPUT_WIDTH], interpolation=PIL.Image.BILINEAR),
+                                            get_transform('rgb', image_size=None)])
+    
+    img_tensor = trans_totensor(img)[:3].unsqueeze(0).to(device)
+
+    if img_tensor.shape[1] == 1:
+        img_tensor = img_tensor.repeat_interleave(3,1)
+
+    output = model(img_tensor).clamp(min=0, max=1)
+
+    if args.task == 'depth':
+
+        output = F.interpolate(output.unsqueeze(0), (HEIGHT, WIDTH), mode='bicubic').squeeze(0)
+        output = output.clamp(0,1)
+        np.save(os.path.join(args.output_path, f'{output_file_name}_depth.npy'), output.to('cpu').numpy())
+
+        # # output = 1 - output
+        # plt.imsave(save_path, output.detach().cpu().squeeze(),cmap='viridis')
+
+        # output = output.detach().cpu().squeeze().numpy()
+        # output = (output - output.min()) / (output.max() - output.min())
+        # np.save(npy_path, output)
+    else:
+        output = F.interpolate(output, (HEIGHT, WIDTH), mode='bilinear') # range [0, 1]
+        output = output * 2. - 1. # range [-1, 1]
+
+        # output = torch.nn.functional.normalize(output * 2 - 1, dim=1)
+        # np.save(os.path.join(args.output_path, f'{output_file_name}_normal.npy'), output.to('cpu').numpy())
+        
+        output = output.squeeze(0) # (3, H, W)
+        output = output.permute(1, 2, 0) # (H, W, 3)
+    
+        R = torch.tensor([
+            [1, 0, 0],
+            [0, -1, 0],
+            [0, 0, -1],
+        ], dtype=torch.float, device=output.device)
+        output = output @ R
+
+        output_img = (1 + output) * 0.5 # range [0, 1]
+        output_img = (output_img * 255).cpu().numpy().astype(np.uint8)
+        Image.fromarray(output_img, mode="RGB").save(os.path.join(args.output_path, f"{output_file_name}.png"))
+        
+        # trans_topil(output[0]).save(save_path)
+        # output = output[0].permute(1,2,0).detach().cpu().numpy()
+        # output = output * 2. - 1.
+        # output = output / (np.linalg.norm(output, axis=-1)[..., None] + 1e-15)
+        # np.save(npy_path, output)
+        # raise NotImplementedError
+
+img_path = Path(args.img_path)
+if img_path.is_dir():
+    images = glob.glob(args.img_path+'/*.jpg') + glob.glob(args.img_path+'/*.png') + glob.glob(args.img_path+'/*.JPG')
+    for f in tqdm(images, desc=f"Estimating {args.task}"):
+        save_outputs(f, os.path.splitext(os.path.basename(f))[0])
+else:
+    raise ValueError("Invalid input path!")
+        
\ No newline at end of file
